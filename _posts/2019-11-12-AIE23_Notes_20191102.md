---
layout: post
title:  "AI Study Notes - Nov 02<sup>nd</sup> 2019"
date:   2019-11-12 23:00:18 +0800
categories: AI
tags: AI Machinelearning Python
author: Hank.S
---

**关键词**  
*特征工程*  
*模型加载和存储*   
*特征选择*   
*降维*   
*可视化*   



---
* content
{:toc}





# 1. Python机器学习 
## 主流python库和框架
- 通用机器学习   
  - Scikit-learn, 开源，涵盖分类，回归和聚类算法   
  - Spark MLlib
- 数据处理
  - Pandas，数据结构DataFrame   
- 科学计算
  - Numpy，矩阵数据类型，矢量处理  
- 可视化 
  - Matplotlib   
- 特定系列算法  
  - XGBoost
- 深度学习
  - TensorFlow, Keras

## 整体流程   
<center><img width="650px" alt="AI知识结构" src="https://ftp.bmp.ovh/imgs/2019/11/66f9ca0c252ff893.png" /></center>

# 2.特征工程
***“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”***   
*特征工程目的是最大限度地从原始数据中提取特征以供算法和模型使用*


## 特征和模型概述   
- 特征和模型的关系
  - 简单特征 <=> 复杂模型
  - 复杂特征 <=> 简单模型
- 特征工程解决的问题
  - 加工数据为模型可处理的格式，即向量化和规范化
  - 某些方法能加速一些优化算法的收敛性
  - 让数据变得使模型更容易拟合
- 特征工程主要分为三部分：
  1) **数据预处理** 对应的sklearn包：[sklearn-Processing data](http://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation)   
  2) **特征选择** 对应的sklearn包： [sklearn-Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)   
  3) **降维** 对应的sklearn包： [sklearn-Dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)  

- 特征工程思考流程
  - 探索：
    - 特征状况：类型，空值，分布（特征选择），异常点，量纲
    - 标签状况：类型，分布
  - 清洗 / 向量化
    - 类别特征编码转为向量: **Onehot**
    - 空值处理
  - 标准化
    - 异常点
    - 量纲是否需要统一
    - 是否需要归一化
    - 分桶离散
	- 离群点处理
  - 特征选择 (过拟合后看看)
    - Filter
    - Wrapper
    - Embed
  - 特征扩展 （欠拟合后看看）
    - 业务总结
    - 组合已有特征 （组合，聚合）  

## 数据预处理([Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling))  


### A.空值填充
- 填固定值
- 离散型填众数，连续型填均值    

```
from __future__ import print_function
import pandas as pd
import numpy as np

dates = pd.date_range('20130101', periods=6)
df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])
df.iloc[0,1] = np.nan
df.iloc[1,2] = np.nan
df.iloc[5,0] = np.nan
df.iloc[4,3] = np.nan
print(df)
print('\n----------------fillna mean--------------------')
print(df.columns[df.isnull().sum(axis=0)>0])
for col in list(df.columns[df.isnull().sum(axis=0)>0]):
    df[col].fillna(value=df[col].mean(), inplace=True)
print(df)
```
- ML 预测 
- 如果空值数量很多，可以丢弃   

### B. 无量纲化
- 标准化（Z-score standardization）
Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-\overline{X}}{S}) 


```
# 参数解释：
# X：数组或者矩阵
# axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations. 如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。
# with_mean: boolean类型，默认为True，表示将数据均值规范到0
# with_std: boolean类型，默认为True，表示将数据方差规范到1
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
X_train = np.arange(1,10,1).reshape(3,3)
X_test = X_train*10
print(X_train)
X_scaled1 = preprocessing.scale(X_train)
X_scaled2 = preprocessing.scale(X_train,axis=1)
print('\n---------------X_scaled1---------------------')
print(X_scaled1)
print('\n---------------X_scaled2---------------------')
print(X_scaled2)
print('\n---------------X_scaled2.mean---------------------')
print(X_scaled2.mean())
print('\n---------------X_scaled2---------------------')
print(X_scaled2.std())
print('\n-----------------StandardScaler-------------------')
st_scaler = StandardScaler(with_std=False).fit(X_train)
print("st_scaler.mean_",st_scaler.mean_)
print("st_scaler.scale_",st_scaler.scale_)
print('\n---------------transform(X_train)---------------------')
print(st_scaler.transform(X_train))
print("X_test",X_test)
print('\n----------------transform(X_test)--------------------')
print(st_scaler.transform(X_test))
```

- 区间缩放 (Scaling features to a range)  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-Min}{Max-Min}) 
```
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```
- 归一化   
Normalization is the process of scaling individual samples to have unit norm.
需求：
  1) 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度
  2) 一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖(比如这时实际情况是值域范围小的特征更重要)。
```
from __future__ import print_function 
from sklearn.preprocessing import Normalizer
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = Normalizer()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```

- 使用场景
  - 如果对输出结果范围有要求，用归一化。  
  - 如果数据较为稳定，不存在极端的最大最小值，用归一化。   
  - 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。


### C. 离散化
- K-bins discretization   

```
import numpy as np
import sklearn.preprocessing as preprocessing

X = np.array([[-3., 5., 15],
              [-1., 6., 14],
              [6., 3., 11]])
est_onehot = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='onehot-dense').fit(X)
est_ordinal = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)
print("est_onehot", est_onehot.transform(X), sep='\n')
print("est_ordinal", est_ordinal.transform(X), sep='\n')
```   

- 对定量特征二值化(Feature binarization)（对列向量处理）
信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。   

```
from sklearn.preprocessing import Binarizer
import numpy as np 
X = np.array([[-3., 5., 15],
              [-1., 6., 14],
              [6., 3., -11]])
binarizer = Binarizer().fit(X)  # fit does nothing
print(binarizer.transform(X))
```  

### Encoding categorical features   
- 独热码(one-hot-enconding)  
将一个特征按其值的不同进行二进制编码，比如性别特征列sex可以转成2列sex_male和sex_femle,每列只有0，1两种数值  

**sklearn.preprocessing.OneHotEncoder**   
```
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
print(enc.n_values_)
print(enc.feature_indices_)
print(enc.transform([[0, 1, 1]]).toarray())
```
 
- 对定性特征哑编码（对列向量处理）  

**Pandas.get_dummies**   
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
data_train = pd.read_csv("data/fix_data_tai1.csv")
dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')
dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')
dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')
dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')
df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)
df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)

df.to_csv("data/fix_data_tai2.csv")
```


### 缺失值计算（对列向量处理） [参考](https://scikit-learn.org/stable/modules/impute.html#impute)

### 数据变换  
- 多项式变换（对行向量处理）
- 自定义变换


&nbsp;

---
## 特征选择  
当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：
- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。
根据特征选择的形式又可以将特征选择方法分为3种：


&nbsp;

---


