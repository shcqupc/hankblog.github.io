---
layout: post
title:  "AI Study Notes 3 -  Feature Engineering"
date:   2019-11-12 23:00:18 +0800
categories: AI
tags: AI Machinelearning Python
author: Hank.S
---

**关键词**  
*特征工程*  
*模型加载和存储*   
*特征选择*   
*降维*   
*可视化*   



---
* content
{:toc}





# 1. Python机器学习 
## 主流python库和框架
- 通用机器学习   
  - Scikit-learn, 开源，涵盖分类，回归和聚类算法   
  - Spark MLlib
- 数据处理
  - Pandas，数据结构DataFrame   
- 科学计算
  - Numpy，矩阵数据类型，矢量处理  
- 可视化 
  - Matplotlib   
- 特定系列算法  
  - XGBoost
- 深度学习
  - TensorFlow, Keras

## 整体流程   
<center><img width="650px" alt="AI知识结构" src="https://ftp.bmp.ovh/imgs/2019/11/66f9ca0c252ff893.png" /></center>

&nbsp;

---

# 2.特征工程
***“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”***   
*特征工程目的是最大限度地从原始数据中提取特征以供算法和模型使用*


## 特征和模型概述   
- 特征和模型的关系
  - 简单特征 <=> 复杂模型
  - 复杂特征 <=> 简单模型
- 特征工程解决的问题
  - 加工数据为模型可处理的格式，即向量化和规范化
  - 某些方法能加速一些优化算法的收敛性
  - 让数据变得使模型更容易拟合
- 特征工程主要分为三部分：
  1) **数据预处理** 对应的sklearn包：[sklearn-Processing data](http://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation)   
  2) **特征选择** 对应的sklearn包： [sklearn-Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)   
  3) **降维** 对应的sklearn包： [sklearn-Dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)  

- 特征工程思考流程
  - 探索：
    - 特征状况：类型，空值，分布（特征选择），异常点，量纲
    - 标签状况：类型，分布
  - 清洗 / 向量化
    - 类别特征编码转为向量: **Onehot**
    - 空值处理
  - 标准化
    - 异常点
    - 量纲是否需要统一
    - 是否需要归一化
    - 分桶离散
	- 离群点处理
  - 特征选择 (过拟合后看看)
    - Filter
    - Wrapper
    - Embed
  - 特征扩展 （欠拟合后看看）
    - 业务总结
    - 组合已有特征 （组合，聚合）  

## 数据预处理([Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling))  


### A.空值填充
- 填固定值
- 离散型填众数，连续型填均值    

```
from __future__ import print_function
import pandas as pd
import numpy as np

dates = pd.date_range('20130101', periods=6)
df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])
df.iloc[0,1] = np.nan
df.iloc[1,2] = np.nan
df.iloc[5,0] = np.nan
df.iloc[4,3] = np.nan
print(df)
print('\n----------------fillna mean--------------------')
print(df.columns[df.isnull().sum(axis=0)>0])
for col in list(df.columns[df.isnull().sum(axis=0)>0]):
    df[col].fillna(value=df[col].mean(), inplace=True)
print(df)
```
- ML 预测 
- 如果空值数量很多，可以丢弃   

### B. 无量纲化
- 标准化（Z-score standardization）
Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-\overline{X}}{S}) 


```
# 参数解释：
# X：数组或者矩阵
# axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations. 如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。
# with_mean: boolean类型，默认为True，表示将数据均值规范到0
# with_std: boolean类型，默认为True，表示将数据方差规范到1
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
X_train = np.arange(1,10,1).reshape(3,3)
X_test = X_train*10
print(X_train)
X_scaled1 = preprocessing.scale(X_train)
X_scaled2 = preprocessing.scale(X_train,axis=1)
print('\n---------------X_scaled1---------------------')
print(X_scaled1)
print('\n---------------X_scaled2---------------------')
print(X_scaled2)
print('\n---------------X_scaled2.mean---------------------')
print(X_scaled2.mean())
print('\n---------------X_scaled2---------------------')
print(X_scaled2.std())
print('\n-----------------StandardScaler-------------------')
st_scaler = StandardScaler(with_std=False).fit(X_train)
print("st_scaler.mean_",st_scaler.mean_)
print("st_scaler.scale_",st_scaler.scale_)
print('\n---------------transform(X_train)---------------------')
print(st_scaler.transform(X_train))
print("X_test",X_test)
print('\n----------------transform(X_test)--------------------')
print(st_scaler.transform(X_test))
```

- 区间缩放 (Scaling features to a range)  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-Min}{Max-Min}) 
```
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```  

- 归一化   
Normalization is the process of scaling individual samples to have unit norm.
需求：
  1) 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度
  2) 一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖(比如这时实际情况是值域范围小的特征更重要)。
```
from __future__ import print_function 
from sklearn.preprocessing import Normalizer
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = Normalizer()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```

- 使用场景
  - 如果对输出结果范围有要求，用归一化。  
  - 如果数据较为稳定，不存在极端的最大最小值，用归一化。   
  - 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。


### C. 离散化
- 分桶离散(K-bins discretization)   

```
import numpy as np
import sklearn.preprocessing as preprocessing

X = np.array([[-3., 5., 15],
              [-1., 6., 14],
              [6., 3., 11]])
est_onehot = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='onehot-dense').fit(X)
est_ordinal = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)
print("est_onehot", est_onehot.transform(X), sep='\n')
print("est_ordinal", est_ordinal.transform(X), sep='\n')
```   
- 分桶离散([pandas.cut](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html?highlight=cut#pandas.cut))  
-    
```
import pandas as pd
import numpy as np
x = np.array([1, 7, 5, 4, 6, 3])
print(pd.cut(x, 3))
[(-0.007, 2.333], (4.667, 7.0], (4.667, 7.0], (2.333, 4.667], (4.667, 7.0], (2.333, 4.667]]
Categories (3, interval[float64]): [(-0.007, 2.333] < (2.333, 4.667] < (4.667, 7.0]]
```  


- 对定量特征二值化(Feature binarization)（对列向量处理）
信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。   

```
from sklearn.preprocessing import Binarizer
import numpy as np

Y = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]
X = np.array([[-3., 5., 15],
              [-1., 6., 14],
              [6., 3., -11]])
binarizer = Binarizer().fit(X)  # fit does nothing
print(binarizer.transform(X))
print(binarizer.transform(Y))
```  

--

### D. Encoding categorical features   
- 独热码(one-hot-enconding)  
将一个特征按其值的不同进行二进制编码，比如性别特征列sex可以转成2列sex_male和sex_femle,每列只有0，1两种数值  

**sklearn.preprocessing.OneHotEncoder**   
```
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
print(enc.n_values_)
print(enc.feature_indices_)
print(enc.transform([[0, 1, 1]]).toarray())
```
 
- 对定性特征哑编码（对列向量处理）  

**Pandas.get_dummies**   
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
data_train = pd.read_csv("data/fix_data_tai1.csv")
dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')
dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')
dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')
dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')
df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)
df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)

df.to_csv("data/fix_data_tai2.csv")
```


### E. 缺失值计算（对列向量处理） [参考](https://scikit-learn.org/stable/modules/impute.html#impute)  

- Univariate feature imputation   

```
from sklearn.datasets import load_iris
from numpy import vstack, array, nan
import numpy as np
from sklearn.impute import SimpleImputer 

iris = load_iris()
print(iris.keys())
imp1 = SimpleImputer(missing_values=np.nan,  strategy='constant', fill_value=888)
imp2 = SimpleImputer(missing_values=np.nan,  strategy='mean')
iris2 = vstack((array([nan, nan, nan, nan]), iris.data))
imp1.fit(iris2)
imp2.fit(iris2)
print('\n---------------constant---------------------')
print(imp1.transform(iris2[:5,:]))
print('\n---------------mean---------------------')
print(imp2.transform(iris2[:5,:]))
```   

- Multivariate feature imputation   
A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. 
This estimator is still experimental for now.   

```
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp = IterativeImputer(max_iter=10, random_state=0)
X = [[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]]
imp.fit(X)
"estimator : estimator object, default=BayesianRidge()"
IterativeImputer(add_indicator=False, estimator=None,
                 imputation_order='ascending', initial_strategy='mean',
                 max_iter=10, max_value=None, min_value=None,
                 missing_values=np.nan, n_nearest_features=None,
                 random_state=0, sample_posterior=False, tol=0.001,
                 verbose=0)

X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
print(np.round(imp.transform(X)))
print(np.round(imp.transform(X_test)))
```

### F. 数据变换  
- 多项式变换（对行向量处理）(Generating polynomial features)  
信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。  

```
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
"(X1,X2) to (1,X1,X2,X1^2,X1*X2,X2^2)"
X = np.arange(9).reshape(3, 3)
print(X[0:5])

poly = PolynomialFeatures(2)
print(poly.fit_transform(X[0:5]))
```

- 自定义变换(Custom transformers)   

```
import numpy as np
from sklearn.preprocessing import FunctionTransformer
ftr= FunctionTransformer(np.log1p, validate=True)
X = np.array([[0, 1], [2, 3]])
print(ftr.transform(X))
```


&nbsp;

---
## 特征选择 [参考](https://scikit-learn.org/stable/modules/feature_selection.html)  
当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：
- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。
根据特征选择的形式又可以将特征选择方法分为3种：  

### 1) Filter
过滤法，不用考虑后续学习器，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。  
a. 方差选择法（Removing features with low variance）  

VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. 

```
from sklearn.feature_selection import VarianceThreshold
import numpy as np
from sklearn.datasets import load_iris
iris = load_iris()
print(iris.data[0:5])
print(VarianceThreshold(threshold=3).fit_transform(iris.data)[0:5])
selector = VarianceThreshold(threshold=0.6).fit(iris.data, iris.target)
data = selector.transform(iris.data)
print(data[0:5])
print(np.var(data,axis=0))
print(np.var(iris.data,axis=0))
print(selector.variances_)
```

b. 卡方检验（Chi-Square Test）[参考](https://www.jianshu.com/p/807b2c2bfd9b)   

```
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.datasets import load_iris
iris = load_iris()
selector = SelectKBest(chi2, k=2).fit(iris.data, iris.target)
data = selector.transform(iris.data)
print(data[0:5])
print(selector.scores_)
```

### 2) Wrapper
a. 递归特征消除法（Recursive feature elimination）  

```
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
iris = load_iris()
# 递归特征消除法，返回特征选择后的数据
# 递归特征消除法
# 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：
# 参数estimator为基模型
# 参数n_features_to_select为选择的特征个数7
print(iris.data[0:5])
selector = RFE(estimator=LogisticRegression(), n_features_to_select=2).fit(iris.data, iris.target)
data = selector.transform(iris.data)
print(data[0:5])
print(selector.ranking_)
```

### 3) Embeded
a. 基于惩罚项的特征选择法   
使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。  
```
1 from sklearn.feature_selection import SelectFromModel
2 from sklearn.linear_model import LogisticRegression
3 
4 # 带L1惩罚项的逻辑回归作为基模型的特征选择
5 SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
```


b. 基于树模型的特征选择法   
树模型中GBDT可用来作为基模型进行特征选择，使用featureselection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：  

```
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import GradientBoostingClassifier 
from sklearn.datasets import load_iris
iris = load_iris()

# GBDT作为基模型的特征选择
# print(SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target))
selector = SelectFromModel(GradientBoostingClassifier()).fit(iris.data, iris.target)
print(iris.data[0:5])
data = selector.transform(iris.data)
print(data[0:5])
print(selector.estimator_.feature_importances_)
```  

### 总结


|类 | 所属方式 | 说明|
| -------- | -------- | -------- |
|VarianceThreshold	|Filter	|方差选择法
|SelectKBest	|Filter	|可选关联系数、卡方校验、最大信息系数作为得分计算的方法|
|RFE	|Wrapper	|递归地训练基模型，将权值系数较小的特征从特征集合中消除|
|SelectFromModel	|Embedded	|训练基模型，选择权值系数较高的特征|

&nbsp;

---

## 了解降维   
降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。  

1. 主成分分析法PCA(Principal Component Analysis)   

```
from sklearn.decomposition import PCA
# 主成分分析法，返回降维后的数据
# 参数n_components为主成分数目
PCA(n_components=2).fit_transform(iris.data)
```

2. 线性判别分析法LDA(Linear Discriminant Analysis)

```
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA 
# 线性判别分析法，返回降维后的数据
# 参数n_components为降维后的维数
LDA(n_components=2).fit_transform(iris.data, iris.target)
```

&nbsp;

---


## 模型加载   

### 1. pickle
```
import pickle
# save
with open('a4_model_load/model/save/clf.pickle', 'wb') as f:
    pickle.dump(clf, f)
# restore
with open('a4_model_load/model/save/clf.pickle', 'rb') as f:
   clf2 = pickle.load(f)
   print(clf2.predict(X[0:1]))
```

### 2. joblib   

```
from sklearn.externals import joblib
# Save
joblib.dump(clf, 'a4_model_load/model/save/clf.pkl')
# restore
clf3 = joblib.load('a4_model_load/model/save/clf.pkl')
print(clf3.predict(X[0:1]))
```


