---
layout: post
title:  "AI Study Notes - Nov 02<sup>nd</sup> 2019"
date:   2019-11-12 23:00:18 +0800
categories: AI
tags: AI Machinelearning Python
author: Hank.S
---

**关键词**  
*特征工程*  
*模型加载和存储*   
*特征选择*   
*降维*   
*可视化*   



---
* content
{:toc}





# 1. Python机器学习 
## 主流python库和框架
- 通用机器学习   
  - Scikit-learn, 开源，涵盖分类，回归和聚类算法   
  - Spark MLlib
- 数据处理
  - Pandas，数据结构DataFrame   
- 科学计算
  - Numpy，矩阵数据类型，矢量处理  
- 可视化 
  - Matplotlib   
- 特定系列算法  
  - XGBoost
- 深度学习
  - TensorFlow, Keras

## 整体流程   
<center><img width="650px" alt="AI知识结构" src="https://ftp.bmp.ovh/imgs/2019/11/66f9ca0c252ff893.png" /></center>

# 2.特征工程
***“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”***   
*特征工程目的是最大限度地从原始数据中提取特征以供算法和模型使用*


## 特征和模型概述   
- 特征和模型的关系
  - 简单特征 <=> 复杂模型
  - 复杂特征 <=> 简单模型
- 特征工程解决的问题
  - 加工数据为模型可处理的格式，即向量化和规范化
  - 某些方法能加速一些优化算法的收敛性
  - 让数据变得使模型更容易拟合
- 特征工程思考流程
  - 探索：
    - 特征状况：类型，空值，分布（特征选择），异常点，量纲
    - 标签状况：类型，分布
  - 清洗 / 向量化
    - 类别特征编码转为向量: **Onehot**
    - 空值处理
  - 标准化
    - 异常点
    - 量纲是否需要统一
    - 是否需要归一化
    - 分桶离散
	- 离群点处理
  - 特征选择 (过拟合后看看)
    - Filter
    - Wrapper
    - Embed
  - 特征扩展 （欠拟合后看看）
    - 业务总结
    - 组合已有特征 （组合，聚合）  

## 数据预处理 （[参考](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)）


### 空值填充
- 填固定值
- 离散型填众数，连续型填均值
```
from __future__ import print_function
import pandas as pd
import numpy as np

dates = pd.date_range('20130101', periods=6)
df = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=['A', 'B', 'C', 'D'])
df.iloc[0,1] = np.nan
df.iloc[1,2] = np.nan
df.iloc[5,0] = np.nan
df.iloc[4,3] = np.nan
print(df)
print('\n----------------fillna mean--------------------')
print(df.columns[df.isnull().sum(axis=0)>0])
for col in list(df.columns[df.isnull().sum(axis=0)>0]):
    df[col].fillna(value=df[col].mean(), inplace=True)
print(df)
```
- ML 预测 
- 如果空值数量很多，可以丢弃   

### 无量纲化
- 标准化（Z-score standardization）
Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-\overline{X}}{S}) 

```
from sklearn import preprocessing
import numpy as np

# The function scale provides a quick and easy way to perform this operation on a single array-like dataset:
X_train = np.array([[1., -1., 2.],
                    [2., 0., 0.],
                    [0., 1., -1.]])
X_scaled = preprocessing.scale(X_train)
print('\n--------------scaled----------------------')
print(X_scaled)
print('\n--------------feature mean----------------------')
print(X_scaled.mean(axis=0))
print('\n--------------feature std----------------------')
print(X_scaled.std(axis=0))

"The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. "
scaler = StandardScaler()
print(scaler.fit(X_train))
print(scaler.transform(X_train)) 
```

- 区间缩放 (Scaling features to a range)  
![](http://latex.codecogs.com/gif.latex?x' =\frac{x-Min}{Max-Min}) 
```
from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```
- 归一化   
Normalization is the process of scaling individual samples to have unit norm.
需求：
  1) 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度
  2) 一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖(比如这时实际情况是值域范围小的特征更重要)。
```
from __future__ import print_function 
from sklearn.preprocessing import Normalizer
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = Normalizer()
print(scaler.fit(data))
print(scaler.transform(data))
print(scaler.transform([[2, 2]]))
```

- 使用场景
  - 如果对输出结果范围有要求，用归一化。  
  - 如果数据较为稳定，不存在极端的最大最小值，用归一化。   
  - 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。


### 对定量特征二值化（对列向量处理）
信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。
```
from sklearn.preprocessing import Binarizer 
X = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]
binarizer = Binarizer().fit(X)  # fit does nothing
print(binarizer.transform(X))
```
### 对定性特征哑编码（对列向量处理）
*独热码(one-hot-enconding)*  
- 将一个特征按其值的不同进行二进制编码，比如性别特征列sex可以转成2列sex_male和sex_femle,每列只有0，1两种数值
- 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。 

**Pandas**   
```
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
data_train = pd.read_csv("data/fix_data_tai1.csv")
dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')
dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')
dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')
dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')
df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)
df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)

df.to_csv("data/fix_data_tai2.csv")
```
**sklearn**   
```
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
print(enc.n_values_)
print(enc.feature_indices_)
print(enc.transform([[0, 1, 1]]).toarray())
```

&nbsp;

---
## 特征选择  


&nbsp;

---


