---
layout: post
title:  "AI Study Notes 5 - Decision Tree"
date:   2019-12-22 22:40:18 +0800
categories: AI
tags: AI Machinelearning Python Decision Tree
author: Hank.S
---

**关键词**  
*Decision Tree*  
*Random Forest*   
*Boost*   
*GBDT*
*集成学习*
*XGBoost*





---
* content
{:toc}




# 1.决策树算法
## 决策树的特性  
  - 可解释性
  - 输入：数据集
  - 输出：创建好的决策树（i.e.训练集）
  - 算法流程(pseudo code)：  
```
def create DT
  if all sample of dataset have same class
    create leaf note with class label
  else
    寻找划分数据集最好的特征
    根据最好的特征划分数据集
    for 每个划分的数据集:
      创建决策子树(递归)
```  
## ID3算法  
- [REF1](https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/) [REF2](https://www.cnblogs.com/kuaizifeng/p/9110157.html)
- 相关公式    
 - Entropy: ![](http://latex.codecogs.com/gif.latex?H(D)=-\sum_{k=1}^{K}\frac{\rvert C_k \rvert}{\rvert D \rvert}log_2\frac{\rvert C_k \rvert}{\rvert D \rvert})  
 - Conditional Entropy: ![](http://latex.codecogs.com/gif.latex?H(D\rvert A)=\sum_{i=1}^{n}\frac{\rvert D_i \rvert}{\rvert D \rvert}H(D_i)=-\sum_{i=1}^{n}\frac{\rvert D_i \rvert}{\rvert D \rvert}\sum_{k=1}^{K}\frac{\rvert D_{ik} \rvert}{\rvert D_i \rvert}log_2\frac{\rvert D_{ik} \rvert}{\rvert D_i \rvert})
 - 信息增益(gain)：g(D,A) = H(D) - H(D\|A)

- 分类算法
- Drawback:
  - Attributes must be nominal values
  - Dataset must not include missing data
  - Tend to fall into overfitting  

```
import numpy as np
import pandas as pd

tennis = pd.read_csv("data/tennis_decision.csv")
print(tennis.columns)
print(tennis.groupby("Decision").Decision.count())  
# equivalent to: select dicision,count(*) from tennis group by decision
#####################################################
# Step 1. Calculate Entropy for decision H(D)
#####################################################
D = -1 * 5 / 14 * np.log2(5 / 14) - 1 * 9 / 14 * np.log2(9 / 14)
print("H(D):", D)
print('\n------------------------------------')
#####################################################
# Step 2. Wind factor on decision g(D,A) = H(D)-H(D|A)
#####################################################
windcount = tennis.groupby(["Wind", "Decision"])["Wind", "Decision"].count()
print(windcount.rename_axis(["Wind_L", "Decision_L"]).sort_values(by=["Wind_L"], ascending=False))
weekwind = -1 * 2 / 8 * np.log2(2 / 8) - 1 * 6 / 8 * np.log2(6 / 8)  # Entropy(Decision|Wind=Week)
strongwind = -1 * 3 / 6 * np.log2(3 / 6) - 1 * 3 / 6 * np.log2(3 / 6)  # Entropy(Decision|Wind=Strong)
gwind = D - (8 / 14 * weekwind + 6 / 14 * strongwind)
print("gwind:", gwind)
print('\n------------------------------------')

#####################################################
# Step 3. Other factors on decision
#####################################################
# humicount = tennis.groupby(["Humidity", "Decision"])["Humidity"].count()
# print(humicount.rename_axis(["Humidity_L", "Decision_L"]))
highhumi = -1 * 4 / 7 * np.log2(4 / 7) - 1 * 3 / 7 * np.log2(3 / 7)  # Entropy(Decision|Humidity=High)
normalhumi = -1 * 1 / 7 * np.log2(1 / 7) - 1 * 6 / 7 * np.log2(6 / 7)  # Entropy(Decision|Humidity=Normal)
ghumi = D - (7 / 14 * highhumi + 7 / 14 * normalhumi)
print("ghumi:", ghumi)

# tempcount = tennis.groupby(["Temp.", "Decision"])["Temp."].count()
# print(tempcount.rename_axis(["Temp_L", "Decision_L"]))
cooltemp = -1 * 1 / 4 * np.log2(1 / 4) - 1 * 3 / 4 * np.log2(3 / 4)  # Entropy(Decision|Temp=Cool)
hottemp = -1 * 2 / 4 * np.log2(2 / 4) - 1 * 2 / 4 * np.log2(2 / 4)  # Entropy(Decision|Temp=Hot)
mildtemp = -1 * 2 / 6 * np.log2(2 / 6) - 1 * 4 / 6 * np.log2(4 / 6)  # Entropy(Decision|Temp=Mild)
gtemp = D - (4 / 14 * cooltemp + 4 / 14 * hottemp + 6 / 14 * mildtemp)
print("gtemp:", gtemp)

# Outlookcount = tennis.groupby(["Outlook", "Decision"])["Outlook"].count()
# print(Outlookcount.rename_axis(["Outlook_L", "Decision_L"]))
OvercastOutlook = -1 * 4 / 4 * np.log2(4 / 4)  # Entropy(Decision|Outlook=Overcast)
RainOutlook = -1 * 2 / 5 * np.log2(2 / 5) - 1 * 3 / 5 * np.log2(3 / 5)  # Entropy(Decision|Outlook=Rain)
SunnyOutlook = -1 * 3 / 5 * np.log2(3 / 5) - 1 * 2 / 5 * np.log2(2 / 5)  # Entropy(Decision|Outlook=Sunny)
gOutlook = D - (4 / 14 * OvercastOutlook + 5 / 14 * RainOutlook + 5 / 14 * SunnyOutlook)
print("gOutlook:", gOutlook)
print('\n------------------------------------')
#####################################################
# outlook decision appears in the root node of the tree
#                         Outlook
#                  /         |        \
#            Sunny(?)  Overcast(Yes)  Rain(?)
#####################################################

#####################################################
# Step 4 Calculate gain on rest features for Sunny day
#####################################################
# Gain(Outlook=Sunny|Wind)
sunnyOutlook = -1 * 3 / 5 * np.log2(3 / 5) - 1 * 2 / 5 * np.log2(2 / 5)  # Entropy(Outlook=Sunny)
SunnyStrongWind = -1 * 1 / 2 * np.log2(1 / 2) - 1 * 1 / 2 * np.log2(1 / 2)  # Entropy(Outlook=Sunny|Wind=Strong)
SunnyWeakWind = -1 * 2 / 3 * np.log2(2 / 3) - 1 * 1 / 3 * np.log2(1 / 3)  # Entropy(Outlook=Sunny|Wind=Week)
gSunnyWind = sunnyOutlook - (2 / 5 * SunnyStrongWind + 3 / 5 * SunnyWeakWind)
print("gSunnyWind:", gSunnyWind)
# Gain(Outlook=Sunny|Humidity)
sunnyHumidity = -1 * 3 / 5 * np.log2(3 / 5) - 1 * 2 / 5 * np.log2(2 / 5)  # Entropy(Outlook=Sunny|Humidity)
sunnyHighHumidity = 0
sunnyNormalHumidity = 0
gSunnyHumidity = sunnyHumidity
print("gSunnyHumidity:", gSunnyHumidity)
# Gain(Outlook=Sunny|Temperature)
# tempcount = tennis[tennis["Outlook"] == "Sunny"].groupby(["Temp.", "Decision"])["Temp."].count()
# print(tempcount.rename_axis(["Temp_L", "Decision_L"]))
SunnyCoolTem = 0  # Entropy(Outlook=Sunny|Temp=Cool)
SunnyHotTem = 0  # Entropy(Outlook=Sunny|Temp=Hot)
SunnyMildTem = -1 * 1 / 2 * np.log2(1 / 2) - 1 * 1 / 2 * np.log2(1 / 2)  # Entropy(Outlook=Sunny|Temp=Mild)
gSunnyTemp = sunnyOutlook - (1 / 5 * SunnyCoolTem + 2 / 5 * SunnyHotTem + 2 / 5 * SunnyMildTem)
print("gSunnyTemp:", gSunnyTemp)

#####################################################
# Humidity decision appears in the Sunny branch node of the tree
#                         Outlook
#            Sunny /         |        \
#            Humidity  Overcast(Yes)  Rain(?)
#####################################################
print('\n------------------------------------')
#####################################################
# Step 5 Calculate gain on rest features for Rain day
#####################################################
# Gain(Outlook=Rain|Wind)
rainoutlook = -1 * 3 / 5 * np.log2(3 / 5) - 1 * 2 / 5 * np.log2(2 / 5)  # Entropy(Outlook = Rain)
rainWeekWind = 0  # Entropy(Outlook=Sunny|Wind=Week)
rainStrongWind = 0  # Entropy(Outlook=Sunny|Wind=Strong)
grainWind = rainoutlook
print("grainWind:", grainWind)
# Gain(Outlook=Rain|Temp)
# tempcount = tennis[tennis["Outlook"] == "Rain"].groupby(["Temp.", "Decision"])["Temp."].count()
# print(tempcount.rename_axis(["Temp_L", "Decision_L"]))
rainCoolTemp = -1 * 1 / 2 * np.log2(1 / 2) - 1 * 1 / 2 * np.log2(1 / 2)
rainMildTemp = -1 * 1 / 3 * np.log2(1 / 3) - 1 * 2 / 3 * np.log2(2 / 3)
grainTemp = rainoutlook - (2 / 5 * rainCoolTemp + 3 / 5 * rainMildTemp)
print("grainTemp:", grainTemp)

```
![](https://i.loli.net/2019/12/30/dLOIux8oGKpbmDi.png)   
Final Version



## 衍生算法
**最佳分类可以用非纯度(impurity)来衡量**  
*掌握算法序列，而不是具体某一个算法*

- C4.5 [REF1](https://blog.csdn.net/zhongranxu/article/details/81910388) [REF2](https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/)
  - 信息增益比(gain ratio)
  - 分类算法
  - Could handle missing data
  - Including continuous data

- CART [REF1](https://blog.csdn.net/e15273/article/details/79648502) [REF2](https://sefiks.com/2018/08/27/a-step-by-step-cart-decision-tree-example/)
  - Both classification and regression
  - GINI系数
  - Seems easier than ID3 and C4.5

### 剪枝算法
- 预剪枝  
  - 在构造决策树的同时进行剪枝  
  - 所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，
为了避免过拟合，可以设定一个阈值。  
  - 例如：熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。
- 后剪枝  
  - 决策树构造完成后进行剪枝   
  - 剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加
量是否小于某一阈值  
  - 如果满足阈值要求，则这一组节点可以合并一个节点，其中包含了所有可能的结果

### 后剪枝-示例
- Reduced - Error Pruning (REP)错误率降低剪枝   
  • 思路：决策树过度拟合后，通过测试数据集来纠正。  
  • 步骤：
  1. 对于完树中每一个非叶子节点的子树，尝试着把它替换成一个叶子节点
  2. 该叶子节点的类别用子树覆盖训练样本中存在最多的那个类来代替，产生简化决策树
  3. 然后比较这两个决策树在测试数据集中的表现
  4. 简化决策树在测试数据集中的错误比较少，那么该子树就可以替换成叶子节点
  5. 以bottom-up的方式遍历所有的子树，当没有任何子树可以替换提升，算法终止



&nbsp;

---

# 2.随机森林  
[REF](https://builtin.com/data-science/random-forest-algorithm)  

&nbsp;

---

# Boost算法
## Adaboost algorithm [REF](https://sefiks.com/2018/11/02/a-step-by-step-adaboost-example/)
 - The main principle in adaboost is to increase the weight of unclassified ones and to decrease the weight value of classified ones. 

## GBDT (Gradient Boosting Decision Tree)[REF](https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/)
