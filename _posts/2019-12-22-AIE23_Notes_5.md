---
layout: post
title:  "AI Study Notes 5 - Decision Tree"
date:   2019-12-22 22:40:18 +0800
categories: AI
tags: AI Machinelearning Python Decision Tree
author: Hank.S
---

**关键词**  
*Decision Tree*  
*Random Forest*   
*Boost*   
*GBDT*
*集成学习*
*XGBoost*





---
* content
{:toc}




# 1.决策树算法
## 决策树的特性：
  - 可解释性
  - 输入：数据集
  - 输出：创建好的决策树（i.e.训练集）
  - 算法流程(pseudo code)：  
  ```
  def create DT
    if all sample of dataset have same class
      create leaf note with class label
    else
      寻找划分数据集最好的特征
      根据最好的特征划分数据集
      for 每个划分的数据集:
        创建决策子树(递归)
  ```
## 衍生算法
**最佳分类可以用非纯度(impurity)来衡量**  
*掌握算法序列，而不是具体某一个算法*
- [ID3](https://www.cnblogs.com/kuaizifeng/p/9110157.html)（信息增益）：分类  
- [C4.5](https://blog.csdn.net/zhongranxu/article/details/81910388) （信息增益比）：分类  
- [CART](https://blog.csdn.net/e15273/article/details/79648502)（GINI系数）：分类与回归  

### 剪枝算法
- 预剪枝  
  - 在构造决策树的同时进行剪枝  
  - 所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，
为了避免过拟合，可以设定一个阈值。  
  - 例如：熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。
- 后剪枝  

&nbsp;

---

# 2.Tensorflow basic
## matrix operation    

## 多层感知器模型   
***多层感知器(Multi-Layer Perceptron,MLP)神经网络即多层全连接神经网络模型***  
- 万能近似定理
  - 有一个隐藏层的多层感知器，可以拟合任意复杂的函数。
- 全连接层   
  - ![](http://latex.codecogs.com/gif.latex?h_{m*n}=f(X_{m*s}*W_{s*n}+b))  
  - f为激活函数   
  - m是样本数量   
  - W的n是特征数量，即隐藏层的神经元数量   
  - h的n代表隐藏层向量的长度
- 深度和广度区别   
  - 增加深度指增加连接层（神经网络）的数量
  - 增加广度指增加W的特征数量，即神经元数量
- 迭代过程中学习率影响   
  - 学习率应该要随着梯度的下降逐渐减小
  - [梯度消失和梯度爆炸](https://www.jianshu.com/p/3f35e555d5ba)



&nbsp;

---

## 分类问题（以手写数字为例）   
### 知识点  
- MNIST (Mixed National Institute of Standards and Technology) database
  - MNIST [官网](http://yann.lecun.com/exdb/mnist/)   
  - [数据格式](https://corochann.com/mnist-dataset-introduction-1138.html)：[样本数量,28,28,1] 需要转换成： [样本数量,784]   
  - 建立模型：线性模型->y=xw+b
- 损失函数
  - 均方误差 (通常都是回归问题)
    - [均方差](https://blog.csdn.net/cqfdcw/article/details/78173839)
  - [交叉熵](https://blog.csdn.net/tsyccnh/article/details/79163834)(通常都是分类问题)
    - 通过[softmax](https://zhuanlan.zhihu.com/p/25723112)将网络输出转为概率q
    - 计算交叉熵 loss = -sum[p*log(q)]
- argmax 是求向量最大值的位置， 如[1000,10]会返回1000个数值
- 模型的存储和读取
- [训练过程的参数分布图](https://www.cnblogs.com/lyc-seu/p/8647792.html)
- 模型复杂度：可训练参数数量
- 是否有其他方法提升精度？
  - 第一个选择：增加模型复杂度->模型完成特征工程(推荐)
  - 第二个选择：增加特征->人工特征 [哈尔小波](https://www.cnblogs.com/wxl845235800/p/10711858.html)
- 用多层神经网络怎么获取W的值？  


### 代码  


## 梯度消失问题（以鸢尾花为例）
- 训练样本数量与可训练参数数量要在同一量级
- ReLU可以在一定程度上解决梯度问题。
- 回忆：数据如何作预处理
    - 去均值：x=x-mean(x)
    - 归一化：x=x/(std(x)+1e-6)
- BatchNorm: 对于每一层数据均进行归一化处理
    - 去均值：x=x-mean(x)
    - 归一化：x=x/(std(x)+1e-6)
    - 两个可训练参数：x = alpha * x + beta
- 现在很多网络中都使用BatchNorm来加快训练速度。
- ReLU+BatchNorm在大部分网络中都是有效的。

```
import pandas as pd
import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np


def variable_summaries(var, name="layer"):
    with tf.variable_scope(name):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)

data = pd.read_csv("../data/iris.data.csv")
c_name = set(data.name.values)
print(c_name)
iris_label = np.zeros([len(data.name.values),len(c_name)])
iris_data = data.values[:, :-1]
iris_data = iris_data-np.mean(iris_data, axis=0)
iris_data = iris_data/np.max(iris_data, axis=0)
train_data=[]
train_data_label=[]
test_data=[]
test_data_label=[]
for idx, itr_name in enumerate(c_name):
    datas_t = iris_data[data.name.values==itr_name, :]
    labels_t = np.zeros([len(datas_t), len(c_name)])
    labels_t[:, idx] = 1
    train_data.append(datas_t[:30])
    train_data_label.append(labels_t[:30])
    test_data.append(datas_t[30:])
    test_data_label.append(labels_t[30:])
train_data = np.concatenate(train_data)
train_data_label = np.concatenate(train_data_label)
test_data = np.concatenate(test_data)
test_data_label = np.concatenate(test_data_label)
x = tf.placeholder(tf.float32, [None, 4], name="input_x")
label = tf.placeholder(tf.float32, [None, 3], name="input_y")
# 对于sigmoid激活函数而言，效果可能并不理想
net = tf.layers.dense(x, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
net = tf.layers.dense(net, 28, activation=tf.nn.sigmoid)
net = tf.layers.batch_normalization(net, training=True)
net = tf.nn.sigmoid(net)
y = tf.layers.dense(net, 3, activation=tf.nn.sigmoid)
loss = tf.reduce_mean(tf.square(y-label))

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
optimizer = tf.train.GradientDescentOptimizer(0.5)
var_list_w = [var for var in tf.trainable_variables() if "kernel" in var.name]
var_list_b = [var for var in tf.trainable_variables() if "bias" in var.name]
gradient_w = optimizer.compute_gradients(loss, var_list=var_list_w)
gradient_b = optimizer.compute_gradients(loss, var_list=var_list_b)
for idx, itr_g in enumerate(gradient_w):
    variable_summaries(itr_g[0], "layer%d-w-grad"%idx)
for idx, itr_g in enumerate(gradient_b):
    variable_summaries(itr_g[0], "layer%d-b-grad"%idx)
for idx, itr_g in enumerate(var_list_w):
    variable_summaries(itr_g, "layer%d-w"%idx)
for idx, itr_g in enumerate(var_list_b):
    variable_summaries(itr_g, "layer%d-b"%idx)
train_step = optimizer.apply_gradients(gradient_w+gradient_b)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
train_writer = tf.summary.FileWriter("logdir-bn", sess.graph)
merged = tf.summary.merge_all()
for itr in range(600):
    sess.run(train_step, feed_dict={x: train_data, label: train_data_label})
    if itr % 30 == 0:
        acc1 = sess.run(accuracy, feed_dict={x: train_data,
                                        label: train_data_label})
        acc2 = sess.run(accuracy, feed_dict={x: test_data,
                                        label: test_data_label})
        print("step:{:6d}  train:{:.3f} test:{:.3f}".format(itr, acc1, acc2))
        summary = sess.run(merged,
                           feed_dict={x: train_data,
                                        label: train_data_label})
        train_writer.add_summary(summary, itr)
```  

## 深度学习缺点
- 容易过拟合：对数据量要求很高
  - 原因
    - 由于神经网络拟合效果很好，拟合曲线将训练集全部覆盖到，反而降低了曲线的容错性。
  - 解决方法
    - 增加训练和测试数据集
    - 数据集要和可训练参数在同一个数量级
- 容易出现梯度消失
- 计算速度慢  


## 样本均衡问题（贷款欺诈为例）
- 正样本 280,000+  负样本400+
- 平衡数据集
- 如果直接将数据输入进行训练
- 需要做样本均衡
  - 采样
  - 加权
