---
layout: post
title:  "AI Study Notes 4 - Dabble with TensorFlow"
date:   2019-12-07 22:40:18 +0800
categories: AI
tags: AI Machinelearning Python TensorFlow
author: Hank.S
---

**关键词**  
*TensorFlow*  
*多层感知模型*   
*手写数字识别*   
*梯度消失问题*





---
* content
{:toc}





# 1. Tensorflow   
## matrix operation    

```
import tensorflow as tf
import numpy as np

a1 = tf.constant(np.ones([4, 4]) * 2, dtype=tf.float64, name="a1")
a2 = tf.Variable(np.ones([4, 4]), dtype=tf.float64, name="a2")
a1_X_a2 = tf.multiply(a1, a2, name='a1_X_a2')
print("a1_X_a2\n", a1_X_a2)
print('\n------------------------------------')
a3 = tf.zeros([4, 4], dtype=tf.float64, name="a3")
a4 = tf.ones([4, 4], dtype=tf.float64, name="a3")
d3_by_d4 = a3 * a4
print("d3_by_d4\n", d3_by_d4)
print('\n--------------matmul----------------------')
def matmul():
    a5 = tf.reshape(a3, [8, 2], name="a5")
    a6 = tf.reshape(a4, [2, 8], name="a6")
    a5_dot_a6 = tf.matmul(a5, a6, name='a5_dot_a6').numpy()
    print("a5_dot_a6\n", a5_dot_a6)   
matmul()    
print('\n--------------tensordot----------------------')
def tensordot():
    a7 = tf.random.uniform((2, 3), maxval=10, dtype=tf.int64, name="a7")
    a8 = tf.random.uniform((1, 3), maxval=10, dtype=tf.int64, name="a8")
    print(a7,a8,sep="\n")
    a7_dot_a8 = tf.tensordot(a7,a8,name="a7_dot_a8",axes=0).numpy()
    print("a7_dot_a8\n", a7_dot_a8)
    print(a7_dot_a8.shape)
tensordot()
print('\n--------------concat----------------------')
a9 = tf.random.uniform([3,3], maxval=10, dtype=tf.int64, name="a9")
a10 = tf.random.uniform([2,3], maxval=5, dtype=tf.int64, name="a10")
a11 = tf.random.uniform([3,2], maxval=5, dtype=tf.int64, name="a11")
a9_c_a10 = tf.concat([a9,a10],axis=0).numpy()
print(a9_c_a10)
a9_c_a11 = tf.concat([a9,a11],axis=1).numpy()
print(a9_c_a11)
```

## 多层感知器模型   
***多层感知器(Multi-Layer Perceptron,MLP)神经网络即多层全连接神经网络模型***  
- 万能近似定理
  - 有一个隐藏层的多层感知器，可以拟合任意复杂的函数。
- 全连接层   
  - ![](http://latex.codecogs.com/gif.latex?h_{m*n}=f(X_{m*s}*W_{s*n}+b))  
  - f为激活函数   
  - m是样本数量   
  - W的n是特征数量，即隐藏层的神经元数量   
  - h的n代表隐藏层向量的长度
- 深度和广度区别   
  - 增加深度指增加连接层（神经网络）的数量
  - 增加广度指增加W的特征数量，即神经元数量
- 迭代过程中学习率影响   
  - 学习率应该要随着梯度的下降逐渐减小
  - [梯度消失和梯度爆炸](https://www.jianshu.com/p/3f35e555d5ba)

```
import tensorflow.compat.v1 as tf
from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

# 需要从外界接收样本：placeholder
x = tf.placeholder(tf.float32, [None, 1])
d = tf.placeholder(tf.float32, [None, 1])

# 需要定义可训练参数：Variable
# w = tf.get_variable("w", [1, 1])
# b = tf.get_variable("b", [1])
# 需要定义模型: y=xw+b
# y = tf.matmul(x, w) + b

w1 = tf.get_variable("w1", [1, 1000])
b1 = tf.get_variable("b1", [1000])
w2 = tf.get_variable("w2", [1000, 1])
b2 = tf.get_variable("b2", [1])
# 需要定义模型: y=xw+b
################################
# 一个全连接层
# h = tf.matmul(x, w1) + b1
# h = tf.nn.relu(h) #激活函数
################################
# y = tf.matmul(h, w2) + b2

# 将全连接层封装成高层次API
h = tf.layers.dense(
    x,
    1000,
    activation=tf.nn.relu)
y = tf.layers.dense(
    h,
    1,
    activation=None)

# 定义优化函数：loss函数
loss = (y-d)**2
loss = tf.reduce_mean(loss) #均方误差
# 定义优化器
optimizer = tf.train.GradientDescentOptimizer(0.01)
train_step = optimizer.minimize(loss)

# 输入样本进行训练
# 定义会话
sess = tf.Session()
sess.run(tf.global_variables_initializer())

## 读取训练数据
import numpy as np
# file = np.load("homework.npz")
# data_x = file['X']
# data_d = file['d']
data_x = np.random.random( [3000,1]) * 6
# data_x = np.random.normal(0,2, [3000,1])
data_d = np.sin(data_x)

for step in range(1000):
    ind = np.random.randint(0,1000,[32])
    inx = data_x[ind]
    ind = data_d[ind]
    st, ls = sess.run(
        [train_step, loss],
        feed_dict={
            x:data_x,
            d:data_d
        }
    )
    print(ls)

pred_y = sess.run(y, feed_dict={x:data_x})
import matplotlib.pyplot as plt
plt.figure(figsize=[4,3])
plt.scatter(data_x[:, 0], data_d[:, 0], c="y",lw=0.2)
plt.scatter(data_x[:, 0], pred_y[:, 0], c="r",lw=0.2)
plt.show()
```


&nbsp;

---

# 2. 分类问题（以手写数字为例）   
