---
layout: post
title:  "AI Study Notes 4 - Dabble with TensorFlow"
date:   2019-12-07 22:40:18 +0800
categories: AI
tags: AI Machinelearning Python TensorFlow
author: Hank.S
---

**关键词**  
*TensorFlow*  
*多层感知模型*   
*手写数字识别*   
*梯度消失问题*
*样本均衡问题*





---
* content
{:toc}





# 1. Tensorflow   
## matrix operation    

```
import tensorflow as tf
import numpy as np

a1 = tf.constant(np.ones([4, 4]) * 2, dtype=tf.float64, name="a1")
a2 = tf.Variable(np.ones([4, 4]), dtype=tf.float64, name="a2")
a1_X_a2 = tf.multiply(a1, a2, name='a1_X_a2')
print("a1_X_a2\n", a1_X_a2)
print('\n------------------------------------')
a3 = tf.zeros([4, 4], dtype=tf.float64, name="a3")
a4 = tf.ones([4, 4], dtype=tf.float64, name="a3")
d3_by_d4 = a3 * a4
print("d3_by_d4\n", d3_by_d4)
print('\n--------------matmul----------------------')
def matmul():
    a5 = tf.reshape(a3, [8, 2], name="a5")
    a6 = tf.reshape(a4, [2, 8], name="a6")
    a5_dot_a6 = tf.matmul(a5, a6, name='a5_dot_a6').numpy()
    print("a5_dot_a6\n", a5_dot_a6)   
matmul()    
print('\n--------------tensordot----------------------')
def tensordot():
    a7 = tf.random.uniform((2, 3), maxval=10, dtype=tf.int64, name="a7")
    a8 = tf.random.uniform((1, 3), maxval=10, dtype=tf.int64, name="a8")
    print(a7,a8,sep="\n")
    a7_dot_a8 = tf.tensordot(a7,a8,name="a7_dot_a8",axes=0).numpy()
    print("a7_dot_a8\n", a7_dot_a8)
    print(a7_dot_a8.shape)
tensordot()
print('\n--------------concat----------------------')
a9 = tf.random.uniform([3,3], maxval=10, dtype=tf.int64, name="a9")
a10 = tf.random.uniform([2,3], maxval=5, dtype=tf.int64, name="a10")
a11 = tf.random.uniform([3,2], maxval=5, dtype=tf.int64, name="a11")
a9_c_a10 = tf.concat([a9,a10],axis=0).numpy()
print(a9_c_a10)
a9_c_a11 = tf.concat([a9,a11],axis=1).numpy()
print(a9_c_a11)
```

## 多层感知器模型   
***多层感知器(Multi-Layer Perceptron,MLP)神经网络即多层全连接神经网络模型***  
- 万能近似定理
  - 有一个隐藏层的多层感知器，可以拟合任意复杂的函数。
- 全连接层   
  - ![](http://latex.codecogs.com/gif.latex?h_{m*n}=f(X_{m*s}*W_{s*n}+b))  
  - f为激活函数   
  - m是样本数量   
  - W的n是特征数量，即隐藏层的神经元数量   
  - h的n代表隐藏层向量的长度
- 深度和广度区别   
  - 增加深度指增加连接层（神经网络）的数量
  - 增加广度指增加W的特征数量，即神经元数量
- 迭代过程中学习率影响   
  - 学习率应该要随着梯度的下降逐渐减小
  - [梯度消失和梯度爆炸](https://www.jianshu.com/p/3f35e555d5ba)

```
import tensorflow.compat.v1 as tf
from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

# 需要从外界接收样本：placeholder
x = tf.placeholder(tf.float32, [None, 1])
d = tf.placeholder(tf.float32, [None, 1])

# 需要定义可训练参数：Variable
# w = tf.get_variable("w", [1, 1])
# b = tf.get_variable("b", [1])
# 需要定义模型: y=xw+b
# y = tf.matmul(x, w) + b

w1 = tf.get_variable("w1", [1, 1000])
b1 = tf.get_variable("b1", [1000])
w2 = tf.get_variable("w2", [1000, 1])
b2 = tf.get_variable("b2", [1])
# 需要定义模型: y=xw+b
################################
# 一个全连接层
# h = tf.matmul(x, w1) + b1
# h = tf.nn.relu(h) #激活函数
################################
# y = tf.matmul(h, w2) + b2

# 将全连接层封装成高层次API
h = tf.layers.dense(
    x,
    1000,
    activation=tf.nn.relu)
y = tf.layers.dense(
    h,
    1,
    activation=None)

# 定义优化函数：loss函数
loss = (y-d)**2
loss = tf.reduce_mean(loss) #均方误差
# 定义优化器
optimizer = tf.train.GradientDescentOptimizer(0.01)
train_step = optimizer.minimize(loss)

# 输入样本进行训练
# 定义会话
sess = tf.Session()
sess.run(tf.global_variables_initializer())

## 读取训练数据
import numpy as np
# file = np.load("homework.npz")
# data_x = file['X']
# data_d = file['d']
data_x = np.random.random( [3000,1]) * 6
# data_x = np.random.normal(0,2, [3000,1])
data_d = np.sin(data_x)

for step in range(1000):
    ind = np.random.randint(0,1000,[32])
    inx = data_x[ind]
    ind = data_d[ind]
    st, ls = sess.run(
        [train_step, loss],
        feed_dict={
            x:data_x,
            d:data_d
        }
    )
    print(ls)

pred_y = sess.run(y, feed_dict={x:data_x})
import matplotlib.pyplot as plt
plt.figure(figsize=[4,3])
plt.scatter(data_x[:, 0], data_d[:, 0], c="y",lw=0.2)
plt.scatter(data_x[:, 0], pred_y[:, 0], c="r",lw=0.2)
plt.show()
```


&nbsp;

---

# 2. 分类问题（以手写数字为例）   
## 知识点  
- MNIST (Mixed National Institute of Standards and Technology) database
  - MNIST [官网](http://yann.lecun.com/exdb/mnist/)   
  - [数据格式](https://corochann.com/mnist-dataset-introduction-1138.html)：[样本数量,28,28,1] 需要转换成： [样本数量,784]   
  - 建立模型：线性模型->y=xw+b
- 损失函数
  - 均方误差 (通常都是回归问题)
    - [均方差](https://blog.csdn.net/cqfdcw/article/details/78173839)
  - [交叉熵](https://blog.csdn.net/tsyccnh/article/details/79163834)(通常都是分类问题)
    - 通过[softmax](https://zhuanlan.zhihu.com/p/25723112)将网络输出转为概率q
    - 计算交叉熵 loss = -sum[p*log(q)]
- argmax 是求向量最大值的位置， 如[1000,10]会返回1000个数值
- 模型的存储和读取
- [训练过程的参数分布图](https://www.cnblogs.com/lyc-seu/p/8647792.html)
- 模型复杂度：可训练参数数量
- 是否有其他方法提升精度？
  - 第一个选择：增加模型复杂度->模型完成特征工程
  - 第二个选择：增加特征->人工特征 [哈尔小波](https://www.cnblogs.com/wxl845235800/p/10711858.html)
- 用多层神经网络怎么获取W的值？  


## 代码  
```
# 引入库
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
import numpy as np

# 获取数据
mnist = input_data.read_data_sets("data/", one_hot=True)

# 构建网络模型
# x，label分别为图形数据和标签数据
x = tf.placeholder(tf.float32, [None, 784])
label = tf.placeholder(tf.float32, [None, 10])

##########################################################
# 方法1 单层神经网络
# 构建单层网络中的权值和偏置
# W = tf.get_variable("W", [784, 10])
# b = tf.Variable(tf.zeros([10]))
# 本例中sigmoid激活函数 约束输出在0-1之间
# y = tf.nn.sigmoid(tf.matmul(x, W) + b)
# y = tf.matmul(x, W) + b

##########################################################
# 方法2 多层神经网络
# 增加神经网络，增加模型复杂度
# 总的可训练参数个数为 784*64 + 64*64 + 64*10
net = tf.layers.dense(x, 64, activation=tf.nn.leaky_relu)  # W1[784,64]
net = tf.layers.dense(net, 64, activation=tf.nn.leaky_relu)  # W2[64,64]
y = tf.layers.dense(net, 10, activation=None)  # W3[64,10]
print(np.shape(x), np.shape(label), np.shape(y))

##将输出y转换为概率
q = tf.nn.softmax(y)
##交叉熵作为损失函数
out = -label * tf.log(q)
out = tf.reduce_sum(out, axis=1)
loss = tf.reduce_mean(out)

# 用梯度迭代算法
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

# 用于验证
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  ## 类型转换

# 定义会话
sess = tf.Session()
# 初始化所有变量
sess.run(tf.global_variables_initializer())

## 指定一个文件用来保存训练过程的参数分布图
# train_writer = tf.summary.FileWriter("mnist-logdir", sess.graph)

## 读取模型
# saver = tf.train.Saver()
# saver.restore(sess, tf.train.latest_checkpoint('23-mnist'))

#########################迭代过程#############################
for itr in range(3000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, label: batch_ys})
    if itr % 100 == 0:
        print("step:%6d  accuracy:" % itr, sess.run(accuracy, feed_dict={x: mnist.test.images,
                                        label: mnist.test.labels}))
## 存储模型
# saver.save(sess, "23-mnist/new", global_step=itr)
```

# 
