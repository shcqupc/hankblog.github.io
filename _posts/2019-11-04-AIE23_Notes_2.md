---
layout: post
title:  "AI Study Notes 2"
date:   2019-11-04 22:40:18 +0800
categories: AI
tags: AI Machinelearning Python
author: Hank.S
---

**关键词**  
*统计学习方法*  
*概率与统计*  
*机器学习建模*
*梯度下降*
*线性代数*
*仿射变换*
*SVD分解*




---
* content
{:toc}





# 1. 课程教材    
- 《统计学习方法》(重点)
  - 有监督的机器学习：支持向量机、决策树等
  - 无监督的机器学习：混合模型等   
- 《深度学习》
- 《算法导论》
- 计算机体系结构
- 最优化算法
- 现代几何学

 


 

&nbsp;

---
# 2. 机器学习模型描述方式 
## 概率论
  - 可参考[博文](https://www.cnblogs.com/sench/p/9478284.html)
- 随机事件
  - 样本点，样本空间
  - 随机事件的样本空间是离散的->分类问题
  - 连续的->回归问题 
- 随机变量及其分布   
- 均值    
  - ![均值1](http://latex.codecogs.com/gif.latex?\mathbb {E}(X)=\sum_{i} x_i \cdot p_i)   
  - ![均值2](http://latex.codecogs.com/gif.latex?E(X)=\int_a^b x \cdot p(x)dx)

- 数学期望
  - 如果说概率是频率随样本趋于无穷的极限，那么期望就是平均数随样本趋于无穷的极限
  - 意义：用于预测一个随机事件的平均预期情况
  - 离散 ![](http://latex.codecogs.com/gif.latex?E(x)=\sum_{k=1}^n x_kP(x_k))  
  - 连续 ![](http://latex.codecogs.com/gif.latex?E(X)=\int_{-\infty}^{+\infty} xP(x)dx)  
- 方差
  - 总体方差: ![](http://latex.codecogs.com/gif.latex?\sigma^2=\frac{\sum(X-\mu)^2}{N})  
  - 离散型方差: ![](http://latex.codecogs.com/gif.latex?\sigma^2(X)=\sum_i (x_i-\mu(X))^2 p_i)   
  - 连续型方差: ![](http://latex.codecogs.com/gif.latex?\sigma^2(X)=\int_a^b(x-E(X))^2 \cdot p(x)dx)   
- 协方差
  - 衡量两个变量的总体误差，当两个变量相同时即为方差
  - 协方差为0的两个随机变量称为是不相关的
  - ![](http://latex.codecogs.com/gif.latex?Cov(X,Y)=E((X-E(X))(Y-E(Y))))   
- 标准差  
- 相关系数(Pearson 相关系数)
  - 是研究变量之间线性相关程度的量  
  - 简单相关系数： ![](http://latex.codecogs.com/gif.latex?r(X,Y)= \frac {Cov(X,Y)}{\sqrt{Var[X]Var[Y]}})    

- 无偏估计
  - 意义：在多次重复下，它们的平均数接近所估计的参数真值
  - 公式：![](http://latex.codecogs.com/gif.latex?S^2=\frac{1}{n-1} \sum_{i=1}^{n} (X_i-\bar X)^2) 
  - 公式推导[参考](https://blog.csdn.net/qq_16587307/article/details/81328773)
- 信息熵
  - 熵（entropy）指的是体系的混乱的程度
  - 知道均值和方差的情况下，高斯分布是最大熵分布
  - 定义：![](http://latex.codecogs.com/gif.latex?H(x)=-\sum_{i=1}^n p(x_i)log(p(x_i)) )    
  - [参考](https://www.cnblogs.com/IamJiangXiaoKun/p/9455689.html)

## 函数（几何）方式描述 
- 分布
  - 均匀分布
    - 算法：LCG 线性同余发生器（Linear congruential generator）
    - `np.random.random([100000])` 
  - 高斯分布(正态分布)
    - 一维正态分布: ![一维正态分布](http://latex.codecogs.com/gif.latex?f(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2}))
    - 标准正态分布: ![标准正态分布](http://latex.codecogs.com/gif.latex?f(x)=\frac{1}{\sqrt{2\pi}}exp(-\frac{x^2}{2})) 
    - `np.random.normal(0, 1, [10000])`    

## 什么是机器学习
- 机器学习是从数据中发现规律（知识） 
- 用函数、概率去拟合数据的过程  

## 机器学习建模
- 建模即假设数据的分布的过程 i.e. 假设两个数据x1，x2均是高斯分布    
- 机器学习的优化过程 i.e. 计算两类数据的均值和方差求高斯分布形式
- 机器学习模型都是真实模型的**简化**，或者说仅是**近似**

&nbsp;

---

# 3. 函数与优化    
## 积分和偏导数
  - 可以使用计算机代数系统完成公式的计算 
  - Mathematica 
  - [Sympy](https://docs.sympy.org/latest/guide.html) 
  - Matlab (x)->Numpy   
  
## 基于梯度的优化算   
  - 原理：   
![](http://latex.codecogs.com/gif.latex?$\because  f(x_0+\Delta x) \approx f(x_0)+f'(x_0)\Delta x$)     
assume the value of ![](http://latex.codecogs.com/gif.latex?$\Delta x = -\eta f'(x_0)$) &nbsp;&nbsp;![](http://latex.codecogs.com/gif.latex?$(\eta >0)$)    
![](http://latex.codecogs.com/gif.latex?\therefore f(x_0+\Delta x)-f(x_0)\approx -\eta f'(x_0)^2 \leq 0)   
![](http://latex.codecogs.com/gif.latex?\therefore f(x_0+\Delta x) \leq f(x_0))   
assume the value of ![](http://latex.codecogs.com/gif.latex?$x_1=x_0-\eta f'(x_0)$)   
![](http://latex.codecogs.com/gif.latex?\therefore f(x_1) \leq f(x_0))   
iteration...   
assume the value of ![](http://latex.codecogs.com/gif.latex?x_n=x_{n-1}-\eta f'(x_{n-1}))     
![](http://latex.codecogs.com/gif.latex?\therefore f(x_n) \leq f(x_{n-1}))   
  - 求函数的极小值(见练习)   
    - 学习率选择(梯度)
      - 过小的学习率：收敛缓慢 
      - 过大的学习率：迭代发散    
       
## 需要了解的其他优化算法
- 二阶优化算法：牛顿法
- 基于随机过程的优化算法 
  - 遗传算法
  - 模拟退火算法
  - 粒子群优化算法   
  
## 已知数据{(x,d)}，求数据间的关系    
1. 建模，假设函数是线性关系(见练习) 
2. 优化，求解各个参数的具体值
3. 用[损失函数(loss)](https://blog.csdn.net/chkay399/article/details/81878157)评估模型输出与真实数据间差距的函数
4. 求解线性回归问题时的平方损失函数公式：![](http://latex.codecogs.com/gif.latex?loss=(y-d)^2)
5. 对loss函数求各参数的偏导数
6. 用梯度下降算法求出loss函数的极小值，此时的参数值就是我们要找的最接近真实模型的值

&nbsp;

---
# 4.线性代数
## 矩阵的乘法
```
import numpy as np
import matplotlib.pyplot as plt

print('\n---------------3*3 matrix---------------------')
A = np.random.random([3, 3])
print(A.shape)
print('\n--------------matrix by matrix----------------------')
B = np.random.random([3, 3])
# 矩阵相乘
C = np.dot(A, B)
# 哈达玛积
D = A * B
# 单位矩阵
E = np.eye(3)
print(C.shape)
print(D.shape)
print('E:{}'.format(E))
print(D * E)
```
## 仿射变换
```
import numpy as np
import matplotlib.pyplot as plt
x = np.random.random([1000, 2])
print(x)
A = np.array([[5, 0.5], [-0.5, 1]])
# print(A.shape)
y = np.dot(x, A)
# print(y)
plt.scatter(x[:, 0], x[:, 1], c='r')
plt.scatter(y[:, 0], y[:, 1], c='b')
plt.axis("equal")
plt.show()
```
## SVD分解：压缩
- 图像数据格式：[高，宽，通道(RGB),透明度(alpha)]
  - 灰度图像：[高，宽] 
- 奇异值分解（Singular Value Decomposition）[参考](https://www.cnblogs.com/endlesscoding/p/10033527.html)
- 练习3

&nbsp;

---
# 5. 练习
## 1) 求函数![](http://latex.codecogs.com/gif.latex?y=2x_1^2+2*x_1+2*x_2^2)的极小值
```
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# 定义函数
def f(x1, x2):
    return x1 ** 2 + 2 * x1 + 2 * x2 ** 2


# 定义函数的偏导数
def df(x1, x2):
    return 2 * x1 + 2, 4 * x2


x1 = 6
x2 = 6
eta = 0.05  # 步长
# 迭代使函数值不断变小
for step in range(200):
    g1, g2 = df(x1, x2)
    x1 = x1 - eta * g1
    x2 = x2 - eta * g2
    print(step, g1, g2, x1, x2, f(x1, x2))

# 绘制函数的3D曲线
fig = plt.figure(figsize=(4.5,3))
ax = Axes3D(fig)
x1 = np.linspace(-10, 10, 1000)
x2 = np.linspace(-10, 10, 1000)
ax.plot3D(x1, x2, f(x1, x2), 'gray')
plt.show()
```
---
## 2) 已知数据{(x,d)}，求数据间的关系   
```
import numpy as np
import matplotlib.pyplot as plt


print('\n---------------已知数据{(x,d)}，求数据间的关系---------------------')
x = np.random.normal(1, 1, [1000])
d = x ** 2 + 1 + np.random.normal(0, 0.6, [1000])

# plt.scatter(x, d)
# plt.show()


print('\n-----------------建立模型-------------------')
def mode(x, a, b, c):
    return a * x ** 2 + b * x + c


"""梯度函数"""
def grad(x, d, a, b, c):
    y = mode(x, a, b, c)
    return 2 * (y - d) * x ** 2, 2 * (y - d) * x, 2 * (y - d)


a = 0.1
b = 0.2
c = 0.3
eta = 0.02
batch_size = 100
for s in range(20):
    sel_idx = np.random.randint(0, len(x), batch_size)
    inx = x[sel_idx]
    ind = d[sel_idx]
    ga, gb, gc = grad(inx, ind, a, b, c)
    a = a - eta * np.mean(ga)
    b = b - eta * np.mean(gb)
    c = c - eta * np.mean(gc)

xplt = np.linspace(-2, 7, 1000)
yplt = mode(xplt, a, b, c)
plt.scatter(x, d)
plt.plot(xplt, yplt, lw=2, c='purple')
plt.show()
```
## 3) 图像的压缩   
```
import numpy as np
from numpy import linalg as LA
import matplotlib.pyplot as plt
from PIL import Image
print('\n---------------img loading---------------------')
img = plt.imread('Resources/jin.jpg')
img = img[:, :, :]

# 处理alpha通道可以用这种方式
imga = Image.open('Resources/jin.jpg')
imga = imga.convert('RGBA') 
print(np.shape(img))
print('imga',np.shape(imga))
a1, a2, a3 = 0.2989, 0.5870, 0.1140
img_gray = img[:, :, 0] * a1 + img[:, :, 1] * a2 + img[:, :, 2] * a3

U, A, V = LA.svd(img_gray, full_matrices=True)
print(U.shape, A.shape, V.shape)
Lambda = np.diag(A)
print(U.shape, Lambda.shape, V.shape)
re_img = np.dot(np.dot(U[:, :20], Lambda[:20, :20]), V[:20, :])
print('re_img', re_img.shape)

# plt.matshow(img)
# plt.matshow(img_gray, cmap=plt.get_cmap("gray"))
plt.matshow(re_img, cmap=plt.get_cmap("gray"))
plt.imsave('Resources/jin_s.jpg',re_img)
plt.show()
```

## 长期作业
- 优化一个 1000*1000 的矩阵乘法 （c、c++）
- 能比numpy的实现慢50倍 

