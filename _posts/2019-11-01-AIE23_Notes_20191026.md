---
layout: post
title:  "AI Study Notes - Oct 26<sup>th</sup> 2019"
date:   2019-11-01 11:40:18 +0800
categories: AI
tags: AI Machinelearning Python
author: Hank.S
---

**关键词**  

*人工智能*  
*机器学习*  
 
---
**Contents**   
&ensp;&ensp;1. 人工智能相关  
&ensp;&ensp;&ensp;&ensp;1) 知识体系    
&ensp;&ensp;&ensp;&ensp;2) 应用场景   
&ensp;&ensp;2. 机器学习相关  
&ensp;&ensp;&ensp;&ensp;1) 机器学习算法  
&ensp;&ensp;&ensp;&ensp;2) 机器学习库和计算框架    
&ensp;&ensp;&ensp;&ensp;3) 如何进行机器学习    
&ensp;&ensp;3. 练习  



&nbsp;

---
# 1. 人工智能相关  
## 1) 知识体系  
***AI知识结构***
![AI知识结构](https://ftp.bmp.ovh/imgs/2019/11/5470f565c796bbf2.png)
由低到上：  
1-2：有理论和技术支撑，构建算法  
3：基本的问题都转化成分类问题，回归问题等  
4：结合传统算法和规则构建实际的应用  
5：系统构建和竞赛

&nbsp;

++AI技术栈++
![AI技术栈](https://ftp.bmp.ovh/imgs/2019/11/acc9e4412358c566.png)

&nbsp;

---
## 2) 应用场景  
* 智能客服
* 实时视频分析：如交通拥堵预测
* 个性化推荐
* 图像识别
* 征信

&nbsp;

---
# 2. 机器学习相关  
## 1) 机器学习算法
### 分类和回归   
* 基于梯度下降的分类和回归算法
* Tree算法
  - 决策树
  - 随机森林
  - GBDT  
### 聚类   
* K-Means
* K-Means++  
### 其他机器学习算法
* KNN
* 推荐ALS
* SVM
### 深度学习   
* NN (Neural Network)
* CNN (Convolutional Neural Networks)
* RNN (Recurrent Neural Network)

&nbsp;

## 2) 机器学习库和计算框架   
- 特征工程与数据预处理：
Numpy / Pandas / Spark / …  
OpenCV / …  
Jieba / NLTK / Scikit-Learn / genism / …  
Matplotlib  
…  
- ML与DL框架：
Tensorflow / Keras / Slim / …  
Scikit-Learn / XGBoost / …  
Spark Mllib / …  
…  

## 3) 如何进行机器学习
* 问题建模
* 获取数据
* 特征工程
* 模型训练与验证
* 模型诊断与调优
* 线上运行  
* 
&nbsp;

---
# 3. 练习  
## 1) Fizz Buzz Study
```

import numpy as np
from sklearn import linear_model
from sklearn.metrics import mean_squared_error

'''
新问题：
面试官让写个程序来玩Prime. 这是一个游戏。玩家从1数到100，如果数字是质数，那么喊'prime'，如果不是但能被3整除就喊'fizz'，否则就直接说数字。
这个游戏玩起来就像是：1, prime, prime, 4, prime, fizz, ...
'''


# 特征工程构造特征方法：将数字1,2,3 ... 等构造特征（重要影响因素，对预测是number, 'fizz'还是"prime"有帮助的因素）
# 将每个输入的数，表示为一个特征数组（向量），这个特征数组有两个维度。

# 判断是否质数
def is_prime(num):
    if num <= 2:
        return 1
    else:
        for i in range(2, num - 1):
            if num % i == 0:
                return 99
    return 1


def feature_engineer(i):
    return np.array([is_prime(i), i % 3])


# 将需要预测的指标转换为数字方法：将数据的真实值（预测结果）number, "prime", "fizz"
# 分别对应转换为数字 0, 1, 2，这样后续能被计算机处理
def construct_sample_label(i):
    if is_prime(i) == 1:
        return np.array([1])
    elif i % 3 == 0:
        return np.array([2])
    else:
        return np.array([0])


# 生成训练集和测试集数据：我们的面试题目标是预测 1 到 100的情况
# 更加公平的预测，不让分类预测器较早的知道要预测的数据的情况，
# 我们选取101到300这个范围的数作为我们的训练集和测试集。
# Note: 语法说明。 for i in range(101, 300)代表Python中从for循环中遍历取值为i，并
# 赋值将i值输入到feature_engineer函数

# 训练集真题
x_train = np.array([feature_engineer(i) for i in range(101, 300)])
# print(x_train)
y_train = np.array([construct_sample_label(i) for i in range(101, 300)])
# print(y_train)

# 测试集期末考试试卷
x_test = np.array([feature_engineer(i) for i in range(1, 100)])
y_test = np.array([construct_sample_label(i) for i in range(1, 100)])

logistic = linear_model.LogisticRegression(C=0.1)  # 内存创建了 y = f(AX)
logistic.fit(x_train, y_train)  # 训练f，找A

# Returns the mean accuracy on the given test data and labels
# 代表模型精准程度
print('LogisticRegression train score: %f'
      % logistic.score(x_train, y_train))
print('LogisticRegression test score: %f'
      % logistic.score(x_test, y_test))

predict = logistic.predict([feature_engineer(231), feature_engineer(232), feature_engineer(101)])
print(predict)

#将预测映射到对应的结果
def out_transfer(num):
    p = logistic.predict([feature_engineer(num)])
    if p == 1:
        return "prime"
    elif p == 2:
        return "fizz"
    else:
        return str(num)

```

## 2) KNN Study
通过KNN算法对鸢尾花分类的流程
1. 加载鸢尾花数据集
2. 将数据集拆分成训练集和测试集，训练集样本数不能少于测试集  
3. 对测试集的每一个样本，找到其在训练集中的K个最近的邻居  
- 计算测试样本和每个训练样本的欧式距离，并将距离存入数据集
- 对距离数据集按距离从小到大排序
- 返回数据集的前K个数组，即K个邻居  

4. 对测试集的每一个样本，统计其K个邻居的类别  
- 遍历每个样本的K个邻居
- 统计K个邻居中所有类别的标签数量，并存入数据集
- 对该数据集按标签数量从大到小排序
- 返回排序后的第一个数组，即标签数量最多的那个类别  

5. 将统计后的类别存入数据集predictions，即测试集每个样本的预测结果
6. 计算预测结果的精确度
- 遍历测试集的每一个样本
- 比较每个样本的标签和预测结果是否相同
- 返回预测正确总数/测试集总数*100%  


```

import csv
import random
import math
import operator
import numpy as np


# 加载鸢尾花数据集文件并按照一定比例split拆分成训练集trainingSet和测试集testSet
def loadDataset(filename, split, trainingSet=[], testSet=[]):
    with open(filename, 'r') as csvfile:
        lines = csv.reader(csvfile)
        dataset = list(lines)
        for x in range(len(dataset) - 1):
            for y in range(4):
                dataset[x][y] = float(dataset[x][y])  # 数据集前4列转换为浮点型
            if random.random() < split:  # 按传入的比例拆分数据集
                trainingSet.append(dataset[x])
            else:
                testSet.append(dataset[x])


# 扩展问题，可以采用其他的距离度量公式？
# 计算标准化欧式距离
def steuDistance(instance1, instance2, length):
    tst_instance = []
    tra_instance = []
    for x in range(length):
        tst_instance.append(float(instance1[x]))
        tra_instance.append(float(instance2[x]))
    X = np.vstack([tst_instance, tra_instance])
    var = np.var(X, axis=0, ddof=1)
    try:
        dist = np.sqrt(((np.array(tst_instance) - np.array(tra_instance)) ** 2 / var).sum())
        if np.isnan(dist):
            return 0
        else:
            return dist
    except Exception as e:
        return 0


# 计算欧式距离
def euclideanDistance(instance1, instance2, length):
    distance = 0
    for x in range(length):
        # 计算欧式距离
        distance += pow((instance1[x] - instance2[x]), 2)
    return math.sqrt(distance)


def getNeighbors(trainingSet, testInstance, k):
    distances = []
    length = len(testInstance) - 1
    # (1) 计算测试样本和每个训练样本的欧式距离
    for x in range(len(trainingSet)):
        dist = euclideanDistance(testInstance, trainingSet[x], length)
        # dist = steuDistance(testInstance, trainingSet[x], length)
        distances.append((trainingSet[x], dist))
    # (2) 对距离进行排序
    distances.sort(key=operator.itemgetter(1))
    neighbors = []
    # (3) 返回最近的K个邻居
    for x in range(k):
        neighbors.append(distances[x][0])
    return neighbors


def getResponse(neighbors):
    classVotes = {}
    # (1) 遍历K个最近的邻居中每个邻居
    for x in range(len(neighbors)):
        # 统计最近邻居中所有的类别标签数量
        response = neighbors[x][-1]
        if response in classVotes:
            classVotes[response] += 1
        else:
            classVotes[response] = 1
    # print('classVotes.items()',classVotes.items())
    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)
    return sortedVotes[0][0]


def getAccuracy(testSet, predictions):
    correct = 0
    # 遍历每个测试集的元素，计算预测值和真实值是否相等，计算准确度
    for x in range(len(testSet)):
        if testSet[x][-1] == predictions[x]:
            correct += 1
    return (correct / float(len(testSet))) * 100.0


def main():
    # prepare data
    trainingSet = []
    testSet = []
    split = 0.67
    # 按黄金比例拆分数据集
    loadDataset('iris.data', split, trainingSet, testSet)
    print('Train set: ' + repr(len(trainingSet)))
    print('Test set: ' + repr(len(testSet)))
    # generate predictions
    predictions = []
    k = 3
    # (0) 遍历每个测试样本
    for x in range(len(testSet)):
        # (1) 对每个测试样本找到训练集中的最近的K邻居
        neighbors = getNeighbors(trainingSet, testSet[x], k)
        # (2) 统计K个邻居的类别
        result = getResponse(neighbors)
        # (3) 记录结果
        predictions.append(result)
        # print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
    accuracy = getAccuracy(testSet, predictions)
    print('Accuracy: ' + repr(accuracy) + '%')


main()
```














































